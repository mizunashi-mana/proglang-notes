@book{Girard:1989,
author = {Girard, Jean-Yves and Taylor, Paul and Lafont, Yves},
booktitle = {Cambridge University Press},
doi = {10.5555/64805},
isbn = {978-0-521-37181-0},
month = {apr},
publisher = {Cambridge University Press},
title = {{Proofs and Types}},
url = {https://dl.acm.org/doi/book/10.5555/64805},
year = {1989}
}

@article{Rossberg:2014,
abstract = {ML modules are a powerful language mechanism for decomposing programs into reusable components. Unfortunately, they also have a reputation for being “complex” and requiring fancy type theory that is mostly opaque to non-experts. While this reputation is certainly understandable, given the many non-standard methodologies that have been developed in the process of studying modules, we aim here to demonstrate that it is undeserved. To do so, we present a novel formalization of ML modules, which defines their semantics directly by a compositional “elaboration” translation into plain System F $\omega$ (the higher-order polymorphic $\lambda$-calculus). To demonstrate the scalability of our “F-ing” semantics, we use it to define a representative, higher-order ML-style module language, encompassing all the major features of existing ML module dialects (except for recursive modules). We thereby show that ML modules are merely a particular mode of use of System F $\omega$ .},
author = {Rossberg, Andreas and Russo, Claudio and Dreyer, Derek},
doi = {10.1017/S0956796814000264},
issn = {0956-7968},
journal = {Journal of Functional Programming},
month = {sep},
number = {5},
pages = {529--607},
title = {{F-ing modules}},
url = {https://www.cambridge.org/core/product/identifier/S0956796814000264/type/journal{\_}article},
volume = {24},
year = {2014}
}

@article{Selinger:2001,
abstract = {We give a categorical semantics to the call-by-name and call-by-value versions of Parigot's $\lambda$$\mu$-calculus with disjunction types. We introduce the class of control categories, which combine a cartesian-closed structure with a premonoidal structure in the sense of Power and Robinson. We prove, via a categorical structure theorem, that the categorical semantics is equivalent to a CPS semantics in the style of Hofmann and Streicher. We show that the call-by-name $\lambda$$\mu$-calculus forms an internal language for control categories, and that the call-by-value $\lambda$$\mu$-calculus forms an internal language for the dual co-control categories. As a corollary, we obtain a syntactic duality result: there exist syntactic translations between call-by-name and call-by-value that are mutually inverse and preserve the operational semantics. This answers a question of Streicher and Reus. {\textcopyright} 2001, Cambridge University Press. All rights reserved.},
author = {Selinger, Peter},
doi = {10.1017/S096012950000311X},
issn = {09601295},
journal = {Mathematical Structures in Computer Science},
month = {apr},
number = {2},
pages = {207--260},
title = {{Control categories and duality: on the categorical semantics of the lambda-mu calculus}},
url = {http://www.journals.cambridge.org/abstract{\_}S096012950000311X},
volume = {11},
year = {2001}
}

@incollection{Rocheteau:2005,
abstract = {Under the extension of Curry-Howard's correspondence to classical logic, Gentzen's NK and LK systems can be seen as syntaxdirected systems of simple types respectively for Parigot's $\lambda$$\mu$-calculus and Curien-Herbelin's $\lambda$̄$\mu$$\mu$̃-calculus. We aim at showing their computational equivalence. We define translations between these calculi. We prove simulation theorems for an undirected evaluation as well as for call-by-name and call-by-value evaluations. {\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
archivePrefix = {arXiv},
arxivId = {arXiv:0706.1728v1},
author = {Rocheteau, J{\'{e}}r{\^{o}}me},
booktitle = {Term Rewriting and Applications},
doi = {10.1007/978-3-540-32033-3_16},
editor = {Giesl, J{\"u}rgen},
eprint = {arXiv:0706.1728v1},
issn = {03029743},
pages = {204--218},
publisher = {Springer, Berlin, Heidelberg},
title = {{Lambda-Mu-Calculus and Duality: Call-by-Name and Call-by-Value}},
url = {http://link.springer.com/10.1007/978-3-540-32033-3_16},
volume = {3467},
year = {2005}
}

@article{Ford:2004,
abstract = {For decades we have been using Chomsky's generative system of grammars, particularly context-free grammars (CFGs) and regular expressions (REs), to express the syntax of programming languages and protocols. The power of generative grammars to express ambiguity is crucial to their original purpose of modelling natural languages, but this very power makes it unnecessarily difficult both to express and to parse machine-oriented languages using CFGs. Parsing Expression Grammars (PEGs) provide an alternative, recognition-based formal foundation for describing machine-oriented syntax, which solves the ambiguity problem by not introducing ambiguity in the first place. Where CFGs express nondeterministic choice between alternatives, PEGs instead use prioritized choice . PEGs address frequently felt expressiveness limitations of CFGs and REs, simplifying syntax definitions and making it unnecessary to separate their lexical and hierarchical components. A linear-time parser can be built for any PEG, avoiding both the complexity and fickleness of LR parsers and the inefficiency of generalized CFG parsing. While PEGs provide a rich set of operators for constructing grammars, they are reducible to two minimal recognition schemas developed around 1970, TS/TDPL and gTS/GTDPL, which are here proven equivalent in effective recognition power.},
author = {Ford, Bryan},
doi = {10.1145/982962.964011},
issn = {0362-1340},
journal = {ACM SIGPLAN Notices},
keywords = {BNF,Context-free grammars,GTDPL,Lexical analysis,Packrat parsing,Parsing expression grammars,Regular expressions,Scannerless parsing,Syntactic predicates,TDPL,Unified grammars},
month = {jan},
number = {1},
pages = {111--122},
title = {{Parsing Expression Grammars: A Recognition-Based Syntactic Foundation}},
url = {https://dl.acm.org/doi/10.1145/982962.964011},
volume = {39},
year = {2004}
}

@mastersthesis{Ford:2002,
abstract = {Packrat parsing is a novel and practical method for implementing linear-time parsers for grammars defined in Top-Down Parsing Language (TDPL). While TDPL was originally created as a formal model for top-down parsers with backtracking capability, this thesis extends TDPL into a powerful general-purpose notation for describing language syntax, providing a compelling alternative to traditional context-free grammars (CFGs). Common syntactic idioms that cannot be represented concisely in a CFG are...},
author = {Ford, Bryan},
school = {Massachusetts Institute of Technology},
title = {{Packrat Parsing : a Practical Linear-Time Algorithm with Backtracking}},
url = {https://pdos.csail.mit.edu/$\sim$baford/packrat/thesis/thesis.pdf},
year = {2002}
}

@article{Knuth:1965,
abstract = {There has been much recent interest in languages whose grammar is sufficiently simple that an efficient left-to-right parsing algorithm can be mechanically produced from the grammar. In this paper, we define LR(k) grammars, which are perhaps the most general ones of this type, and they provide the basis for understanding all of the special tricks which have been used in the construction of parsing algorithms for languages with simple structure, e.g. algebraic languages. We give algorithms for deciding if a given grammar satisfies the LR(k) condition, for given k, and also give methods for generating recognizes for LR(k) grammars. It is shown that the problem of whether or not a grammar is LR(k) for some k is undecidable, and the paper concludes by establishing various connections between LR(k) grammars and deterministic languages. In particular, the LR(k) condition is a natural analogue, for grammars, of the deterministic condition, for languages. {\textcopyright} 1965 Academic Press, Inc.},
author = {Knuth, Donald E.},
doi = {10.1016/S0019-9958(65)90426-2},
issn = {00199958},
journal = {Information and Control},
month = {dec},
number = {6},
pages = {607--639},
title = {{On the translation of languages from left to right}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0019995865904262},
volume = {8},
year = {1965}
}

@misc{SimonMarlow:2010,
abstract = {In September of 1987 a meeting was held at the conference on Functional Programming Languages and Computer Architecture (FPCA '87) in Portland, Oregon, to discuss an unfortunate situation in the functional programming community: there had come into being more than a dozen non-strict, purely functional programming languages, all similar in expressive power and semantic underpinnings. There was a strong consensus at this meeting that more widespread use of this class of functional languages was being hampered by the lack of a common language. It was decided that a committee should be formed to design such a language, providing faster communication of new ideas, a stable foundation for real applications development, and a vehicle through which others would be encouraged to use functional languages. This document describes the result of that (and subsequent) committee's efforts: a purely functional programming language called Haskell, named after the logician Haskell B. Curry whose work provides the logical basis for much of ours.},
author = {{Simon Marlow}},
institution = {Cambridge},
keywords = {Haskell},
title = {{Haskell 2010 Language Report}},
url = {https://www.haskell.org/onlinereport/haskell2010/},
year = {2010}
}

@article{Mizushima:2008,
   abstract = {Packrat parsing is a parsing technique that combines memoization with backtracking recur-sive descent parser. It can handle any grammars that can be expressed in powerful grammar notation called parsing expression grammar (PEG). Generated parsers can analyze its input in linear time. However, to memoize intermediate result, packrat parsing requires storage area proportional to the input size. In this paper, we propose the packrat parser generator system that disposes unnecessary storage area by adding the notion of a cut operator to PEG. Cut operators is the notion we borrowed from Prolog that can be used by programmers to 'cut off' an alternative execution branch of a choice point in a syntax rule when the alternative should not be tried for suppressing unnecessary backtracking. When an alternative is removed by execution of a cut operator (and thus the parser can make sure that no backtracking can occur before a particular input positon), the memoization storage kept against backtracking can be deallocated and reclaimed dynamically. We believe that, for most pratical grammars, parsers which only use bounded memory size can be generated by appropriate insertion of cut operators in syntax rules. Although memory efficiency that can be achieved by hand-insertion of cut operators would be valuable, it is awesome and error-prone. In this paper, we also propose a technique to reduce required storage area by statically detecting syntax rules which memoization is unnecessary and another technique for automatically inserting cut operators without changing meanings of syntax rules.},
   author = {Kota Mizushima and Atusi Maeda and Yoshinori Yamaguchi},
   issn = {0387-5806},
   issue = {SIG1(PRO35)},
   journal = {情報処理学会論文誌  (IPSJ Journal)},
   pages = {117-126},
   title = {Improvement Technique of Memory Efficiency of Packrat Parsing},
   volume = {49},
   url = {https://jglobal.jst.go.jp/en/detail?JGLOBAL_ID=200902209111853132},
   year = {2008},
}

@inproceedings{Mizushima:2010,
   author = {Kota Mizushima and Atusi Maeda and Yoshinori Yamaguchi},
   city = {New York, New York, USA},
   doi = {10.1145/1806672.1806679},
   isbn = {9781450300827},
   booktitle = {Proceedings of the 9th ACM SIGPLAN-SIGSOFT workshop on Program analysis for software tools and engineering - PASTE '10},
   pages = {29},
   publisher = {ACM Press},
   title = {Packrat parsers can handle practical grammars in mostly constant space},
   url = {http://portal.acm.org/citation.cfm?doid=1806672.1806679},
   year = {2010},
}

@article{Lee:1998,
   abstract = {The Hindley/Milner let-polymorphic type inference system has two different algorithms: one is the de facto standard Algorithm W that is bottom-up (or context-insensitive), and the other is a “folklore” algorithm that is top-down (or context-sensitive). Because the latter algorithm has not been formally presented with its soundness and completeness proofs, and its relation with the W algorithm has not been rigorously investigated, its use in place of (or in combination with) W is not well founded. In this article, we formally define the context-sensitive, top-down type inference algorithm (named “M”), prove its soundness and completeness, and show a distinguishing property that M always stops earlier than W if the input program is ill typed. Our proofs can be seen as theoretical justifications for various type-checking strategies being used in practice.},
   author = {Oukseh Lee and Kwangkeun Yi},
   doi = {10.1145/291891.291892},
   issn = {0164-0925},
   issue = {4},
   journal = {ACM Transactions on Programming Languages and Systems},
   keywords = {D33 [Programming Languages]: Language Constructs and Features-data types and structures; F33 [Logics and Meaning of Programs]: Studies of Program Constructs-type structure General Terms: Algorithms,Languages,Theory Additional Key Words and Phrases: Type error,type inference algorithm},
   month = {7},
   pages = {707-723},
   title = {Proofs about a folklore let-polymorphic type inference algorithm},
   volume = {20},
   url = {https://dl.acm.org/doi/10.1145/291891.291892},
   year = {1998},
}

@incollection{Eekelen:2004,
   author = {Van Eekelen and Van Leer},
   doi = {10.7551/mitpress/1104.003.0016},
   booktitle = {Advanced Topics in Types and Programming Languages},
   publisher = {The MIT Press},
   title = {The Essence of ML Type Inference},
   url = {https://direct.mit.edu/books/book/2718/chapter/73516/the-essence-of-ml-type-inference},
   year = {2004},
   chapter = {10},
}

@misc{Grabmuller:2006,
   abstract = {In this paper we develop a complete implementation of the classic algorithm W for Hindley-Milner polymorphic type inference in Haskell.},
   author = {Martin Grabmüller},
   title = {Algorithm W Step by Step},
   year = {2006},
}

@article{Wells:1999,
   abstract = {Girard and Reynolds independently invented System F (a.k.a. the second-order polymorphically typed lambda calculus) to handle problems in logic and computer programming language design, respectively. Viewing F in the Curry style, which associates types with untyped lambda terms, raises the questions of typability and type checking. Typability asks for a term whether there exists some type it can be given. Type checking asks, for a particular term and type, whether the term can be given that type. The decldabllity of these problems has been settled for restrictions and extensions of F and related systems and complexity lower-bounds have been determined for typability in F, but this report is the first lo resolve whether these problems are decidable for System F. This report proves that type checking in F is undecidable, by a reduction from .srll?i-zrnjfrc.trtiorl, and that typability in F is undecidable, by a reduction from type checking. Because there is an easy reduction from typability to type checking, the two problems are equivalent. The reduction from type checking to typability uses a novel method of constructing lambda terms that simulate arbitrarily chosen type environments. All of the results also hold for the il-calculux. @ 1999 Published by Elsevier Science B.V. All rights t-eserved.},
   author = {J.B. Wells},
   doi = {10.1016/S0168-0072(98)00047-5},
   issn = {01680072},
   issue = {1-3},
   journal = {Annals of Pure and Applied Logic},
   keywords = {KPJWII&: System F,Lambda calculus,Semi-umfication,Typability: Type checking,Type inference},
   month = {6},
   pages = {111-156},
   title = {Typability and type checking in System F are equivalent and undecidable},
   volume = {98},
   url = {https://linkinghub.elsevier.com/retrieve/pii/S0168007298000475},
   year = {1999},
}
